{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis of Functional Connectivity\n",
    "\n",
    "In this tutorial, we'll apply simple graph theoretical techniques to characterize our functional connectivity data we generated in tutorial 10. We'll discuss how our connectivity matrix maps onto a graph, and what types of measures we can pull out of it in order to understand the underlying connectivity structure. \n",
    "\n",
    "Without a doubt, the foundational paper for the analyses we'll perfom here is [Rubinov and Sporns (2010)](https://www.sciencedirect.com/science/article/abs/pii/S105381190901074X?via%3Dihub). They describe a variety of ways in which we can analyze brain networks beyond applying simple statistics to connectivity matrices (e.g., mass univariate testing). Definitely required reading if you are interested in functional connectivity. Another excellent resource is [Fundamentals of Brain Network Analysis](https://www.elsevier.com/books/fundamentals-of-brain-network-analysis/fornito/978-0-12-407908-3) by Bullmore et al.\n",
    "\n",
    "Here we'll also make use of [Networkx](https://networkx.github.io/documentation/stable/index.html), which let's us visualize graphs. In combination with Brain Connectivity Toolbox (BCT), we have a core toolbox for network neuroscience at our disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial, stats, cluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "from nilearn import plotting, input_data\n",
    "from nilearn.datasets import fetch_atlas_schaefer_2018\n",
    "from nilearn import plotting, image\n",
    "import networkx as nx\n",
    "import bct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load in our atlas we used in tutorial 10 (the Schaefer 200 atlas). We'll be using this later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = fetch_atlas_schaefer_2018(n_rois=200, resolution_mm=2)\n",
    "labels = [x.decode() for x in atlas['labels']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Thresholding\n",
    "\n",
    "First, we'll load our connectivity matrix and then threshold it using a proportional threshold of the top 10% of connectivity weights. All of the measures we'll use here work with either weighted or binary matrices (note that some methods that we don't explore only accept binary matrices). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmat = np.loadtxt('connectivity.csv', delimiter=',')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plotting.plot_matrix(cmat, vmax=1, vmin=-1, figure=fig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_cmat = bct.threshold_proportional(cmat, 0.1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plotting.plot_matrix(thresh_cmat, vmax=1, vmin=-1, figure=fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constructing a graph\n",
    "\n",
    "Let's just take the same subnetwork we looked at last week, and also add in some motor regions. This will make life a bit easier for understanding various graph theory concepts and measures. Selecting this set of regions from our thresholded matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = thresh_cmat[:30, :30] # creates a smaller adjacency matrix 'A'\n",
    "\n",
    "# Let's clean up Schaefer atlas labels (e.g., '7Networks_LH_Vis_1') for better visualization.\n",
    "# This list comprehension splits each label by '_', takes the last two parts ('Vis', '1'),\n",
    "# joins them back together (e.g., 'Vis1'), and does this for the first 30 labels.\n",
    "region_labels = [\"\".join(x.split(\"_\")[-2:]) for x in labels[:30]]\n",
    "\n",
    "region_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "# plotting.plot_matrix(A, vmax=1, vmin=-1)\n",
    "plotting.plot_matrix(A, vmax=1, vmin=-1, labels=region_labels, figure=fig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs represent the connectivity structure among the regions in this matrix. Each region is a **node**. And each correlation between regions/nodes are called **edges**; the strength of the correlation is the edge **weight**. \n",
    "\n",
    "First we need to build our graph in networkx (`nx`). We can use our connectivity matrix, called an **adjacency matrix** in graph theory (`A`), to create `G`, our graph. Note that we're generating an **undirected weighted** graph because correlations (weights) are nondirectional. \n",
    "\n",
    "We'll also label our nodes according to the regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_array(A)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what does G contain?\n",
    "list(G.edges(data='weight'))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column above represents the 'source' node, the second column represents the 'target' node, and the third column is the 'weight' (correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relabel the default integer nodes (0, 1, 2...) with our region_labels.\n",
    "# The lambda function maps the old node index `x` to the label at that index in region_labels.\n",
    "G = nx.relabel_nodes(G, lambda x: region_labels[x])\n",
    "print(list(G.edges(data='weight'))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Plotting our graph\n",
    "\n",
    "Now we can plot our graph. A really common way to plot graphs are using force-directed drawing algorithms that try to position nodes such that the edges are roughly equal in length with minimal overlap (see [Wikipedia](https://en.wikipedia.org/wiki/Force-directed_graph_drawing)). Networkx has the Kamada-Kawai algorithm (a widely used algorithm in the field): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of colours to match the network labels, Vis (purple) and SomMot (blue)\n",
    "region_colors = ['purple'] * 14 + ['steelblue'] * 16\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12)) # Adjust (10, 10) as needed\n",
    "nx.draw_kamada_kawai(G, node_color=region_colors, node_size=500, \n",
    "                     with_labels=True, font_color='k', font_size=12) # the basic vanilla version of kamada_kawai ignores the weight information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back and compare this visualization with the connectivity matrix. Does it make sense? Why are some regions close together while others are more spread out?\n",
    "\n",
    "### 2.2 Adding weights\n",
    "\n",
    "Above, we're completely ignoring weights/correlations. We can add these in, but we'll have to program this differently so we can customize it a bit.\n",
    "\n",
    "First, we can extract out our edges and their corresponding weights. This is akin to going through each cell above and getting the corresponding regions, and the correlation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges, weights = zip(*nx.get_edge_attributes(G,'weight').items()) #tuple upnacking -- zip wil produce two tuples (edges and weights)\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_widths = [(.5 + x) ** 4 for x in weights]\n",
    "#This line transforms the raw correlation values (weights) into a list of numerical values (weight_widths) that will \n",
    "# be used to set the visual thickness (width) of the edges in the plot. The specific transformation (+ 0.5, ** 4) is chosen \n",
    "# to make stronger connections appear substantially thicker than weaker ones in an aesthetically informative way.\n",
    "\n",
    "layout = nx.kamada_kawai_layout(G)\n",
    "nx.draw_networkx_nodes(G, layout, node_color=region_colors, node_size=500)\n",
    "nx.draw_networkx_labels(G, layout, font_color='w', font_size=5)\n",
    "nx.draw_networkx_edges(G, layout, edgelist=edges, width=weight_widths)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph measures\n",
    "\n",
    "Graph theory is the study of graphs and their properties. We can use graph theory measures to understand the connectivity structure in our data. For example, what region has the greatest number of connections? What region is the most 'central' to our network? For any two regions, how many other regions are 'in between'?\n",
    "\n",
    "### 3.1 Network density\n",
    "\n",
    "We can measure how dense each network is by dividing the number of edges by the number of possible edges. In other words, **network density** is the fraction of possible edges that are present.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_density, n, k = bct.density_und(A[:14, :14]) #first 14 visual regions\n",
    "mot_density, n, k = bct.density_und(A[14:, 14:]) #remaining regions\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 3))\n",
    "ax.bar(['Vis', 'Mot'], [vis_density, mot_density],\n",
    "       width=.8, color=['purple', 'steelblue'])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at our connectivity matrix or graph, we can confirm these results. More motor regions are connected to one another than visual regions are. I.e., it is a more 'integrated' module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Node degree and strength\n",
    "\n",
    "**Node degree** is simply the number of edges for a node. If we didn't threshold our connectivity matrix, all nodes would have the same degree (i.e. number of regions - 1). However, because we applied a threshold, certain nodes have greater degree than others. We can use BCT to compute the degrees of our nodes:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = bct.degrees_und(A)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.bar(region_labels, degrees, width=.8, color=region_colors)\n",
    "plt.xticks(rotation=90)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a graph layout that changes node size based on degree score..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tweak these values to get a good visual scale ---\n",
    "baseline_size = 5   # Minimum node size (for nodes with centrality near 0)\n",
    "scale_factor = 100  # Multiplier for degree values (increase for larger differences)\n",
    "\n",
    "# Ensure between_cent is a NumPy array for vectorized operation\n",
    "node_sizes = baseline_size + np.array(degrees) * scale_factor\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Draw the nodes using the calculated sizes and specified order\n",
    "# Use nodelist to ensure sizes/colors match the nodes correctly\n",
    "nx.draw_networkx_nodes(G, layout, ax=ax,\n",
    "                       nodelist=region_labels, # Specify node order\n",
    "                       node_size=node_sizes,  # Use the scaled sizes\n",
    "                       node_color=region_colors) # Use the predefined colors\n",
    "\n",
    "# Draw the edges (you could combine this with edge width scaling from section 2.2 if desired)\n",
    "nx.draw_networkx_edges(G, layout, ax=ax, alpha=0.3, width=0.5) # Make edges thinner/lighter\n",
    "\n",
    "# Draw the labels (optional, adjust font size or selectively label)\n",
    "nx.draw_networkx_labels(G, layout, ax=ax, font_size=8, font_color='black')\n",
    "\n",
    "# Add title and turn off axis\n",
    "ax.set_title(\"Kamada-Kawai Layout (Node Size scaled by Degree Score)\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the plot shows, some regions have a higher degree (more connections surviving the threshold) than others within this subnetwork. Regions with high degree are locally well-connected but this doesn't account for the strength of those connections. That is, node degree ignores the connectivity weights (i.e. correlation). **Node strength**, however, is the sum of weights for a node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strengths = bct.strengths_und(A)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.bar(region_labels, strengths, width=.8, color=region_colors)\n",
    "plt.xticks(rotation=90)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this plot above is very closely related to node degree, but the y-axis is different (i.e., it reflects the sum of weights) and there are subtle changes across regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 6)) # Adjust figsize as needed\n",
    "scatter = ax.scatter(degrees, strengths, c=region_colors, s=60, alpha=0.8)\n",
    "\n",
    "#Add labels and title for clarity\n",
    "ax.set_xlabel(\"Node Degree (Number of Connections)\")\n",
    "ax.set_ylabel(\"Node Strength (Sum of Connection Weights)\")\n",
    "ax.set_title(\"Node Strength vs. Node Degree (Subnetwork)\")\n",
    "\n",
    "ax.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Path length\n",
    "\n",
    "A **path** is a series of edges between nodes. The **shortest path** is, as the name implies, the shortest possible path between two nodes. Directly adjacent or connected nodes have a shortest path of 1, and indirectly connected nodes have a shortest path of > 1. Therefore, the shortest path is a measure of distance between pairs of nodes. \n",
    "\n",
    "We can obtain the shortest paths using Dijkstra's algorithm implemented in BCT. First, we'll plot the number of edges that make up the shortest path between each node. In an unweighted (binary) matrix, the number of edges is equivalent to the path length: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the weighted connectivity matrix A into a 'length' or 'distance' matrix.\n",
    "# Shortest path algorithms aim to minimize distance. Since high correlation implies\n",
    "# nodes are 'closer' or more strongly connected, we need to invert the weights\n",
    "# (e.g., length = 1/weight) so that stronger connections have shorter lengths.\n",
    "# BCT's 'lengths' option handles this appropriately.\n",
    "A_inv = bct.weight_conversion(A, 'lengths') #create A_inv which is the distance matrix\n",
    "\n",
    "dist, num = bct.distance_wei(A_inv)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plotting.plot_matrix(num, vmin=0, labels=region_labels, \n",
    "                     figure=fig, cmap='viridis')\n",
    "ax.set_title('Number of edges for shortest path between two nodes');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our matrix is weighted (i.e. has correlations), path length takes the edge strength into account, where stronger correlations are interpreted as shorter paths. In a weighted matrix, the shortest path is the sum of inverse correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plotting.plot_matrix(dist, vmin=0, labels=region_labels, \n",
    "                     figure=fig, cmap='viridis')\n",
    "ax.set_title('Total weighted path length');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to distinguish between `num` and `dist` is that num represents the number of nodes you need to get through to 'talk' to another node whereas dist represents the total distance travelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the average shortest path, otherwise known as the **characteristic path length**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude infinite values (white squares in the 'dist' matrix plot)\n",
    "finite_dist = dist[~np.isinf(dist)] # Get only finite distances\n",
    "# Calculate mean, handle edge case where finite_dist might be empty if graph is disconnected\n",
    "characteristic_path = np.mean(finite_dist) if finite_dist.size > 0 else np.inf\n",
    "print(f\"Characteristic Path Length: {characteristic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could reflect a single brain-wide measure (or whatever your matrix is composed of) that characterizes the potential for cross-talk between brain areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Centrality\n",
    "\n",
    "Centrality speaks to the extent of which a node is 'central' to the network. There are a variety of measures for centrality, but a particularly popular measure is **betweeness centrality**. Betweeness centrality measures the proportion of shortest paths between all node pairs that pass through a node. If many paths pass through a node, that node is deemed central to a network. This will make a lot of sense when you compare these results with the graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the inverse matrix\n",
    "between_cent = bct.betweenness_wei(A_inv)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.bar(region_labels, between_cent, width=.8, color=region_colors)\n",
    "plt.xticks(rotation=90)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and plot this betweeness centrality on our graph to visually depict these relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tweak these values to get a good visual scale ---\n",
    "baseline_size = 5   # Minimum node size (for nodes with centrality near 0)\n",
    "scale_factor = 5  # Multiplier for centrality values (increase for larger differences)\n",
    "\n",
    "# Ensure between_cent is a NumPy array for vectorized operation\n",
    "node_sizes = baseline_size + np.array(between_cent) * scale_factor\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Draw the nodes using the calculated sizes and specified order\n",
    "# Use nodelist to ensure sizes/colors match the nodes correctly\n",
    "nx.draw_networkx_nodes(G, layout, ax=ax,\n",
    "                       nodelist=region_labels, # Specify node order\n",
    "                       node_size=node_sizes,  # Use the scaled sizes\n",
    "                       node_color=region_colors) # Use the predefined colors\n",
    "\n",
    "# Draw the edges (you could combine this with edge width scaling from section 2.2 if desired)\n",
    "nx.draw_networkx_edges(G, layout, ax=ax, alpha=0.3, width=0.5) # Make edges thinner/lighter\n",
    "\n",
    "# Draw the labels (optional, adjust font size or selectively label)\n",
    "nx.draw_networkx_labels(G, layout, ax=ax, font_size=8, font_color='black')\n",
    "\n",
    "# Add title and turn off axis\n",
    "ax.set_title(\"Kamada-Kawai Layout (Node Size scaled by Betweenness Centrality)\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regions with high betweenness centrality lie on many of the shortest paths connecting other pairs of regions. This suggests they might act as important 'bridges' or information relays within this subnetwork.\n",
    "\n",
    "Alternatively, there is the **eigenvector centrality**. Here, nodes are more central if they are connected to other central nodes. We expect more central nodes to be in the center of the network, and therefore connected to one another. As a thought experiment, how might this relate to network density?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigin_cent = bct.eigenvector_centrality_und(A)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.bar(region_labels, eigin_cent, width=.8, color=region_colors)\n",
    "plt.xticks(rotation=90)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Participation Coefficient\n",
    "\n",
    "The **participant coefficient** is simply the number of edges that connect to nodes in different clusters/modules (e.g., brain network). So we can measure how much a brain region 'participates' in other brain networks. If the participation coefficient is high, the brain region connects to many other brain networks. Low participation coefficients mean that a brain region mostly (if not entirely) connects to regions within the network it belongs to. Any guesses as to what region has the highest participation coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label based on Vis or SomMot\n",
    "network_index = [1] * 14 + [2] * 16\n",
    "participation = bct.participation_coef(A, network_index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.bar(region_labels, participation, width=.8, color=region_colors)\n",
    "plt.xticks(rotation=90)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it on the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tweak these values to get a good visual scale ---\n",
    "baseline_size = 50   # Minimum node size (for nodes with centrality near 0)\n",
    "scale_factor = 3000  # Multiplier for participation values (increase for larger differences)\n",
    "\n",
    "# Ensure between_cent is a NumPy array for vectorized operation\n",
    "node_sizes = baseline_size + np.array(participation) * scale_factor\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Draw the nodes using the calculated sizes and specified order\n",
    "# Use nodelist to ensure sizes/colors match the nodes correctly\n",
    "nx.draw_networkx_nodes(G, layout, ax=ax,\n",
    "                       nodelist=region_labels, # Specify node order\n",
    "                       node_size=node_sizes,  # Use the scaled sizes\n",
    "                       node_color=region_colors) # Use the predefined colors\n",
    "\n",
    "# Draw the edges (you could combine this with edge width scaling from section 2.2 if desired)\n",
    "nx.draw_networkx_edges(G, layout, ax=ax, alpha=0.3, width=0.5) # Make edges thinner/lighter\n",
    "\n",
    "# Draw the labels (optional, adjust font size or selectively label)\n",
    "nx.draw_networkx_labels(G, layout, ax=ax, font_size=8, font_color='black')\n",
    "\n",
    "# Add title and turn off axis\n",
    "ax.set_title(\"Kamada-Kawai Layout (Node Size scaled by Partic. Coeff.)\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Modularity\n",
    "\n",
    "Modules refer to subgroups (or clusters) of densely connected nodes. We might expect our pre-defined functional brain networks that we already have (Vis, SomMot) are their own clusters. We can also apply clustering algorithms or dedicated community-detection algorithms to generate our own communities/clusters/modules. \n",
    "\n",
    "We've seen hierarchical clustering before when we did RSA and briefly when we were visualizing connectivity matrices last week. If we apply hierarchical clustering to these data, we get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get upper triangle\n",
    "distances = spatial.distance.squareform(1 - A, checks=False)\n",
    "\n",
    "# apply hierarchical clustering \n",
    "linkages = cluster.hierarchy.linkage(distances, method='average')\n",
    "\n",
    "# plot dendogram\n",
    "dendo = cluster.hierarchy.dendrogram(linkages, labels=region_labels)\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is an agglomerative clustering technique that forms clusters based on the closeness or similarity of nodes within a cluster.\n",
    "\n",
    "with BCT, we can use the generalized Louvain algorithm to estimate communities within the data. The Louvain algorithm is an algorithm that iteratively assigns nodes to different clusters/communities and converges on the solution that gives the highest modularity metric, _Q_. _Q_ indicates the extent to which the network is cleanly delineated into groups. When we run this function, we'll get our module label for each node, and _Q_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_louvain, q = bct.community_louvain(A)\n",
    "print(f\"Q value: {q}\")\n",
    "print(f\"Module_Louvain: {module_louvain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this Q value could change across subjects and/or when comparing different groups (e.g., patients vs. controls).\n",
    "\n",
    "Module_Louvain is the output of the clustering algorithm, and assigns each node to a cluster/module.\n",
    "\n",
    "Now we visualize the generalized Louvain algorithm results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(G, node_color=module_louvain, node_size=500, \n",
    "                     with_labels=True, font_color='w', font_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Whole-brain analysis\n",
    "\n",
    "Now that we've explore some metrics in our subnetwork, we can apply some of them to the whole brain. For simplicity, let's assign each region a number so that we have a reference when visualizing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = pd.DataFrame({'number': np.arange(len(labels)) + 1, 'region': labels})\n",
    "label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Constructing a graph\n",
    "\n",
    "We'll construct a whole-brain graph. Before we do that, just a couple of functions that assigns our nodes a color and network name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_color(x):\n",
    "    \n",
    "    networks = ['Vis', 'SomMot', 'DorsAttn', 'SalVentAttn', \n",
    "                'Limbic', 'Cont', 'Default']\n",
    "    cmap = ['purple', 'steelblue', 'green', 'violet', \n",
    "            'lightgoldenrodyellow', 'orange', 'indianred'] #this uses the classic Yeo network color scheme\n",
    "    pairings = dict(zip(networks, cmap)) #pairs up the networks with the colors and converts to dictionary\n",
    "    \n",
    "    network_label = x.split('_')[2] #splits the string into a list of parts and selects the select element (e.g., Vis)\n",
    "    return pairings[network_label] #grabs the associated color for a given network level\n",
    "\n",
    "def set_network(x):\n",
    "    \n",
    "    networks = ['Vis', 'SomMot', 'DorsAttn', 'SalVentAttn', \n",
    "                'Limbic', 'Cont', 'Default']\n",
    "    index = [1, 2, 3, 4, 5, 6, 7]\n",
    "    pairings = dict(zip(networks, index))\n",
    "    \n",
    "    network_label = x.split('_')[2]\n",
    "    return pairings[network_label] #turns networks into numbers 1-7. We can use this for participation coefficient (below).\n",
    "\n",
    "node_colors = [set_color(i) for i in labels]\n",
    "node_network = [set_network(i) for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_array(thresh_cmat)\n",
    "G = nx.relabel_nodes(G, lambda x: label_data['number'].tolist()[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(G, node_color=node_colors, node_size=150, with_labels=True, font_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about a mess! This kind of looks like a hairball, but you can see that there is structure there. Regions belonging the the same networks, particularly the purple (Vis), blue (SomMot), and Red (Default) tend to stick together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the Schaefer atlas layout is below: \n",
    "\n",
    "![](https://github.com/danjgale/psyc-917/blob/master/images/schaefer_100.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Graph metrics \n",
    "\n",
    "Now we can run some of our graph measures we ran above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change A to entire brain\n",
    "A = thresh_cmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the connectivity distance between regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_inv = bct.weight_conversion(A, 'lengths')\n",
    "\n",
    "dist, num = bct.distance_wei(A_inv)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plotting.plot_matrix(dist, vmin=0,\n",
    "                     figure=fig, cmap='viridis')\n",
    "ax.set_title('Connectivity distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets visualize some stats on the brain. As we've seen, we can compute measures that are properties of a brain region, like the number of connections it has. What does this look like on the brain itself? We'll create a function to plot this data on the brain as a stat map, like we've seen before. This basically maps the region's value to every voxel that belongs to the region, therefore visualizing each region's value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = fetch_atlas_schaefer_2018(n_rois=200, resolution_mm=2)\n",
    "atlas_img = nib.load(atlas['maps'])\n",
    "\n",
    "def stat_map(x, atlas): #x = measure\n",
    "\n",
    "    img_data = atlas.get_fdata()\n",
    "    indices = np.unique(img_data)[1:]\n",
    "\n",
    "    arr = img_data.copy()\n",
    "    for val, i in zip(x, indices):\n",
    "        arr = np.where(arr == i, val, arr)\n",
    "        \n",
    "    return nib.Nifti1Image(arr, atlas.affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compute a bunch of measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = bct.degrees_und(A)\n",
    "strengths = bct.strengths_und(A)\n",
    "eigin_cent = bct.eigenvector_centrality_und(A)\n",
    "between_cent = bct.betweenness_wei(A_inv)\n",
    "\n",
    "participation = bct.participation_coef(A, node_network)\n",
    "module_louvain,Q = bct.community_louvain(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize any of these measures with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = module_louvain # change out with any of the above!\n",
    "\n",
    "res = stat_map(measure, atlas_img)\n",
    "plotting.view_img(res, vmin=0, cmap='viridis', symmetric_cmap=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myneuro] *",
   "language": "python",
   "name": "conda-env-myneuro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
